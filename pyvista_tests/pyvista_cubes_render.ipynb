{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdcb307-7eef-49e2-aa37-eb234e6cb83f",
   "metadata": {},
   "source": [
    "To Do:  \n",
    "* move video rotation.\n",
    "* Legend.\n",
    "* Check dates competibilities color to mesh.\n",
    "* calc clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856896ab-e2dc-4e68-8e1f-788b0064cd04",
   "metadata": {},
   "source": [
    "test the basic functions needed to create the cube rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d693e55d-c244-4054-9492-8600a4f95672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import vtk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from skimage import io\n",
    "import cv2\n",
    "import logging\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import QhullError\n",
    "#import gemgis as gg\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8db744-72de-4a62-891a-98238dfe7b4a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2585bbcb-722d-4514-87c5-c11d0effb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_padding = 400\n",
    "cube_size = 10\n",
    "cmap = 'seismic' #'RdBu'# 'PiYG' 'seismic_r'\n",
    "clim = (-100,100)\n",
    "clim = (0,2.6)\n",
    "crop_coordinates = None # (40,55,0,20)\n",
    "\n",
    "n_timepoints=100\n",
    "frame_rate=30\n",
    "event_csv_path='events.csv'\n",
    "\n",
    "path_input_mesh = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "#path_input_color = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "path_input_color = '../../UFZ_RemoteSensing/NOAA-LAI-Europe-mon-2001-2012(1).nc'\n",
    "\n",
    "output_path=\"output_cube10_seismic_bgPad400.mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46c97879-b318-47d0-936b-de6d6ba3b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_padding = 220\n",
    "cube_size = 20\n",
    "cmap = 'PiYG' #'RdBu'# 'PiYG' 'seismic_r'\n",
    "clim = (-100,100)\n",
    "clim = (0,2.6)\n",
    "crop_coordinates = (40,59,0,21)\n",
    "\n",
    "n_timepoints=20\n",
    "frame_rate=30\n",
    "event_csv_path=None#'events.csv'\n",
    "\n",
    "is_interp = 'spatial'\n",
    "\n",
    "path_input_mesh = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "#path_input_color = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "path_input_color = '../../UFZ_RemoteSensing/NOAA-LAI-Europe-mon-2001-2012(1).nc'\n",
    "\n",
    "output_path=\"output_cube20_PiYG_bgPad200_cropped.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c78c83c-e2ac-4b2a-9c15-0100ebc40be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_BG = '../backgound_map_tests/bluemarble.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb4c97-449f-45e7-879d-ee74721b5e17",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddfc2a-89dc-49f0-bbb5-8da1de432545",
   "metadata": {},
   "source": [
    "### Load, crop & resize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18d059b-1981-4505-b967-8f9eca5adb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc(path_input):\n",
    "    ds = nc.Dataset(path_input)\n",
    "    arr_name = [n for n in ds.variables.keys() if ds[n].ndim==3][0]\n",
    "    arr = ds[arr_name]\n",
    "\n",
    "    lat_arr = ds['latitude'][:]\n",
    "    long_arr = ds['longitude'][:]\n",
    "    time_arr = [s[:-2] for s in ds['time'][:].astype(str)]\n",
    "    \n",
    "    logging.info(f'Read netCDF {arr_name} data. Shape={ds[arr_name].shape}')\n",
    "    \n",
    "    return arr, lat_arr, long_arr, time_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18facf88-f055-41b3-98c4-85ae69be972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arr_2D(arr, arr_slice=0, fill_val=-9999, min_zero=False):\n",
    "    arr = arr[:][arr_slice]\n",
    "    arr = arr.filled(fill_value=np.nan)\n",
    "    arr = arr - np.nanmin(arr) if min_zero else arr\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf58070-929a-456c-9fac-fb9dab810212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(coor_arr_mesh, coor_arr_color, crop_coordinates):\n",
    "    coor_min = np.nanmax((np.nanmin(coor_arr_mesh), np.nanmin(coor_arr_color), crop_coordinates[0]))\n",
    "    coor_max = np.nanmin((np.nanmax(coor_arr_mesh), np.nanmax(coor_arr_color), crop_coordinates[1]))\n",
    "    \n",
    "    return coor_min, coor_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4292634e-7e00-4b6d-9983-fceec9ec23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_nearest(arr, val):\n",
    "    return (np.abs(arr - val)).argmin()\n",
    "\n",
    "def get_world_part(arr, lat_arr_src, long_arr_src, \n",
    "                   lat_arr_dst=None, long_arr_dst=None,\n",
    "                   padding=0):\n",
    "    \n",
    "    if lat_arr_dst is None:\n",
    "        lat_arr_dst = np.linspace(-90, 90, arr.shape[0])\n",
    "        long_arr_dst = np.linspace(-180, 180, arr.shape[1])\n",
    "\n",
    "    min_lat_idx = find_idx_nearest(lat_arr_dst, np.min(lat_arr_src))\n",
    "    max_lat_idx = find_idx_nearest(lat_arr_dst, np.max(lat_arr_src))\n",
    "\n",
    "    min_long_idx = find_idx_nearest(long_arr_dst, np.min(long_arr_src))\n",
    "    max_long_idx = find_idx_nearest(long_arr_dst, np.max(long_arr_src))\n",
    "\n",
    "    arr = arr[min_lat_idx-padding:max_lat_idx+padding, min_long_idx-padding:max_long_idx+padding]\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef53556-8e49-4799-aa1a-ef4050e337f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BG_mesh(path_input_BG, bg_padding, lat_min, lat_max, long_min, long_max):\n",
    "    bg = np.flip(io.imread(path_input_BG), axis=0)\n",
    "    bg = get_world_part(bg, (lat_min, lat_max), (long_min, long_max), padding=bg_padding)\n",
    "    data_shape = [s-bg_padding*2 for s in bg.shape[:2]]\n",
    "    path_bg = 'tmp.png'\n",
    "    io.imsave(path_bg, np.flip(bg,axis=0))\n",
    "    bg = pv.read(path_bg)\n",
    "    bg.origin = (-bg_padding, -bg_padding, 0)\n",
    "    \n",
    "    return bg, data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "10dba6bc-8dc2-4226-9be9-c9b92875fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(arr_color):\n",
    "    \n",
    "    if ~np.all(np.isnan(arr_color)):\n",
    "        \n",
    "        idxs_to_interp = np.where(np.isnan(arr_color))\n",
    "        idxs_interp_from = np.where(~np.isnan(arr_color))\n",
    "        \n",
    "        try:\n",
    "            arr_color[idxs_to_interp] = griddata(idxs_interp_from, \n",
    "                                             arr_color[idxs_interp_from], \n",
    "                                             idxs_to_interp)\n",
    "        except QhullError:\n",
    "            logging.warning('not interpulating - QHull error')\n",
    "            \n",
    "    return arr_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c6d4790d-4cf2-4e78-aa41-db3be7681892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans_temporal(arr_color_ref):\n",
    "    \n",
    "    for idx in range(arr_color_ref.shape[-1]):\n",
    "        arr_color = load_arr_2D(arr_color_ref, np.index_exp[:,:,idx])\n",
    "        interpolate_nans(arr_color)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663e544-5a53-4a0d-8c1c-5df4f2eea7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_color_ref[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fca87-e884-456e-b835-6718b50ae2f1",
   "metadata": {},
   "source": [
    "### Make plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c41fd4-c121-4051-bb88-f8cf2757451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plotter(frame_rate):\n",
    "    p = pv.Plotter(notebook=False, off_screen=True,\n",
    "                  window_size=[1920 * 2, 1080 * 2], \n",
    "                   multi_samples=16, lighting='three lights')\n",
    "    p.open_movie(output_path, framerate=frame_rate)\n",
    "    p.set_background('black')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c29df8-1b83-4276-937b-2448bcf0bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_BG(p, bg):\n",
    "    floor_thickness = 40\n",
    "    p.add_mesh(pv.Cube(center=(bg.center[0], bg.center[1], -floor_thickness), \n",
    "                       x_length=bg.bounds[1]-bg.bounds[0],\n",
    "                       y_length=bg.bounds[3]-bg.bounds[2], \n",
    "                       z_length=floor_thickness*2-0.02),\n",
    "               color='white')\n",
    "\n",
    "    p.add_mesh(bg, rgb=True, scalars='PNGImage')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba8c62-3359-4311-ac0b-9476ec335fc6",
   "metadata": {},
   "source": [
    "### Make mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d8b9247-a581-4d07-871e-5cb1da84e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_floor_vertices(arr, cube_size):\n",
    "\n",
    "    xs,ys = np.meshgrid(range(arr.shape[0]),range(arr.shape[1]))\n",
    "    \n",
    "    vertices_base = np.vstack((xs.T.flatten()*cube_size, \n",
    "                        ys.T.flatten()* cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "    \n",
    "    xs,ys = np.meshgrid(range(arr.shape[0]),[arr.shape[1]])\n",
    "    vertices_edge_y = np.vstack((xs.T.flatten()*cube_size, \n",
    "                        ys.T.flatten()*cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "    \n",
    "    xs,ys = np.meshgrid(arr.shape[0],range(arr.shape[1]))\n",
    "    vertices_edge_x = np.vstack((xs.T.flatten()* cube_size, \n",
    "                        ys.T.flatten()* cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "        \n",
    "    return vertices_base, vertices_edge_y, vertices_edge_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e40e90-86d8-44e9-86f9-380a69879d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all_vertices(arr, vertices_base, vertices_edge_y, vertices_edge_x, cube_size=1):\n",
    "    \n",
    "    vertices_ceil = vertices_base.copy()\n",
    "    vertices_ceil[:,2] = (arr.flatten()/10)**2-(np.nanmin(arr.flatten()/10)**2)\n",
    "    \n",
    "    vertices = np.concatenate((vertices_base,\n",
    "                               vertices_edge_y,\n",
    "                               vertices_edge_x,\n",
    "                               vertices_ceil,\n",
    "                               vertices_ceil+[0,1*cube_size,0],\n",
    "                               vertices_ceil+[1*cube_size,1*cube_size,0],\n",
    "                               vertices_ceil+[1*cube_size,0,0],\n",
    "                          ))\n",
    "    \n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9627dedf-eab8-42c7-8da7-127d29ec3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vertices(arr, cube_size=1):\n",
    "    vertices_base, vertices_edge_y, vertices_edge_x = make_floor_vertices(arr, cube_size)\n",
    "    vertices = concat_all_vertices(arr, vertices_base, vertices_edge_y, vertices_edge_x, cube_size)\n",
    "    \n",
    "    n_floor_v = vertices_base.shape[0]+vertices_edge_y.shape[0]+vertices_edge_x.shape[0]\n",
    "    return vertices, n_floor_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d43a20-a588-43af-80a4-c5057687ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_floor_faces(arr):\n",
    "    faces = []\n",
    "    it = np.nditer(arr, flags=['c_index','multi_index'])\n",
    "\n",
    "    for x in it:\n",
    "        if not np.isnan(x):\n",
    "            \n",
    "            neigh_right_idx = it.index+1 if it.multi_index[1]!=arr.shape[1]-1 \\\n",
    "                                else arr.size + it.multi_index[0] \n",
    "            neigh_down_idx = it.index+arr.shape[1] if it.multi_index[0]!=arr.shape[0]-1 \\\n",
    "                                else arr.size + arr.shape[0] + it.multi_index[1]\n",
    "            \n",
    "            neigh_diag_idx = it.index+arr.shape[1]+1\n",
    "            if neigh_down_idx>arr.size:\n",
    "                neigh_diag_idx = neigh_down_idx+1\n",
    "            elif neigh_right_idx>arr.size-1:\n",
    "                neigh_diag_idx = neigh_right_idx+1\n",
    "\n",
    "            faces.append(np.array([4,\n",
    "                                   it.index,\n",
    "                                   neigh_right_idx,\n",
    "                                   neigh_diag_idx,\n",
    "                                   neigh_down_idx,\n",
    "                                  ]))\n",
    "            \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ed371b-86a9-4a62-b6fe-e4ae687353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ceil_faces(arr, n_v_floor):\n",
    "\n",
    "    faces = []\n",
    "    it = np.nditer(arr, flags=['c_index','multi_index'])\n",
    "\n",
    "    for x in it:\n",
    "        if not np.isnan(x):\n",
    "            faces.append(np.array([4,\n",
    "                                   n_v_floor+it.index,\n",
    "                                   n_v_floor+arr.size+it.index,\n",
    "                                   n_v_floor+arr.size*2+it.index,\n",
    "                                   n_v_floor+arr.size*3+it.index,\n",
    "                                  ]))\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd46421e-9c04-4b8a-abe4-5e51253445a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_side_faces(faces):\n",
    "    side_faces = []\n",
    "\n",
    "    for i in range(len(faces)//2):\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][1],\n",
    "                               faces[i][2],\n",
    "                               faces[len(faces)//2+i][2],\n",
    "                               faces[len(faces)//2+i][1],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][1],\n",
    "                               faces[i][4],\n",
    "                               faces[len(faces)//2+i][4],\n",
    "                               faces[len(faces)//2+i][1],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][2],\n",
    "                               faces[i][3],\n",
    "                               faces[len(faces)//2+i][3],\n",
    "                               faces[len(faces)//2+i][2],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][3],\n",
    "                               faces[i][4],\n",
    "                               faces[len(faces)//2+i][4],\n",
    "                               faces[len(faces)//2+i][3],\n",
    "                              ]))\n",
    "        \n",
    "    return side_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b43e6f-8aae-42c1-a2e0-43f2d34b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_faces(arr, n_floor_v):\n",
    "    faces = make_floor_faces(arr)\n",
    "    faces.extend(make_ceil_faces(arr, n_floor_v))\n",
    "    faces.extend(make_side_faces(faces))\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da98a9eb-abe1-47c8-aa10-b5ba33fba794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_face_color(arr_color_flat, arr_mesh, clim):\n",
    "    arr_color_flat = np.delete(arr_color_flat, np.isnan(arr_mesh.flatten(order='F')))\n",
    "    \n",
    "    faces_color = np.concatenate((np.tile(arr_color_flat, 2),\n",
    "                               np.tile(arr_color_flat, (4,1)).flatten(order='F')))\n",
    "    \n",
    "#     arr_color_opacity = (np.abs(arr_color_flat-np.mean([clim[0],clim[1]])) / (np.ptp(clim)/2))*3\n",
    "    \n",
    "#     faces_opacity = np.concatenate((np.tile(arr_color_opacity, 2),\n",
    "#                                np.tile(arr_color_opacity, (4,1)).flatten(order='F')))    \n",
    "    \n",
    "#     faces_opacity[faces_opacity>1] = 1.0\n",
    "    \n",
    "#     faces_opacity = faces_opacity*0.8\n",
    "    \n",
    "    return faces_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7ce4eb-b435-442b-b370-4963c5c922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_surface(arr_mesh, cube_size, arr_color, clim):\n",
    "    \n",
    "    vertices, n_floor_v = make_vertices(arr_mesh, cube_size)\n",
    "    faces = make_faces(arr_mesh, n_floor_v)\n",
    "    surf = pv.PolyData(vertices, faces)\n",
    "    faces_color = make_face_color(arr_color.flatten(order='F'), arr_mesh, clim)\n",
    "    surf[\"colors\"] = faces_color\n",
    "    \n",
    "    return surf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcfa72-23e9-4734-a7e0-1c114de0e9ea",
   "metadata": {},
   "source": [
    "### Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c5a52c1-ac80-4ceb-9356-cc49c1abfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(event_csv_path, time_arr, data_shape, lat_min, lat_max, long_min, long_max):\n",
    "    \"\"\" Gets a csv file path of highlighted events that should be annotated in the rendering,\n",
    "    (as a text bubble on a specified map location) \n",
    "    it converts the data to x,y mesh position and timeframe indexing and returns it as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        event_csv_path: str path to csv file. Each row in the csv is an event. csv should have 5 columns:\n",
    "            \"first_date\" - first date of event in the format e.g. 20001231\n",
    "            \"last_date\" - last date of event\n",
    "            \"longitude\" - event longitude coordinate\n",
    "            \"latitude\" - event latitude coordinate\n",
    "            \"text\" - event text\n",
    "        time_arr: 1D array. Part of the netCDF dataset\n",
    "        long_arr: 1D array. Part of the netCDF dataset\n",
    "        lat_arr: 1D array. Part of the netCDF dataset\n",
    "\n",
    "    Returns:\n",
    "        event_dict - dict representation of the csv where each key is a timeframe index.\n",
    "                example: {\"0\":[[-20,250,150],\"event text example\"]}\n",
    "                means that there is only one event, it would be displayed at the first (0) timeframe,\n",
    "                at location [x,y,z] of the mesh coordinates.\n",
    "\n",
    "    Note:\n",
    "        for now also processing timepoints outside of user input timepoints.\n",
    "    TODO:\n",
    "        Need to check that the csv format is correct.\n",
    "        for now not checking that long and lat values are close to values in arrays:\n",
    "        e.g. [abs(long_arr[idx_long[i]]-val)<max_dist for i,val in enumerate(loc_arr[0])]\n",
    "    \"\"\"\n",
    "\n",
    "    lat_arr = np.linspace(lat_min, lat_max, data_shape[0])\n",
    "    long_arr = np.linspace(long_min, long_max, data_shape[1])\n",
    "        \n",
    "    event_dict = {}\n",
    "\n",
    "    df = pd.read_csv(event_csv_path, dtype={\"first_date\": str, \"last_date\": str})\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            idx_first_date = time_arr.index(df.loc[i, \"first_date\"])\n",
    "            idx_last_date = time_arr.index(df.loc[i, \"last_date\"])\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            logging.warning(f'event csv has an invalid date - row #{i}')\n",
    "            continue\n",
    "            \n",
    "        point = [(np.abs(long_arr - df.loc[i, \"longitude\"])).argmin(),\n",
    "                 (np.abs(lat_arr - df.loc[i, \"latitude\"])).argmin(),\n",
    "                 150]  # x,y,z\n",
    "        \n",
    "        if (not lat_min<point[0]<lat_max) or (not long_min<point[0]<long_max):\n",
    "            logging.warning(f'event csv has an invalid location - row #{i}')\n",
    "            continue\n",
    "                                    \n",
    "        text = (df.loc[i, \"text\"]).replace('\\\\n', '\\n')\n",
    "        for idx in range(idx_first_date, idx_last_date + 1):\n",
    "\n",
    "            if str(idx) not in event_dict:\n",
    "                event_dict[str(idx)] = [[point], [text]]\n",
    "            else:\n",
    "                event_dict[str(idx)][0].append(point)\n",
    "                event_dict[str(idx)][1].append(text)\n",
    "\n",
    "    return event_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e68da68-7498-4995-81d7-b07c12a44bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_actors(p):\n",
    "    p.remove_actor('mesh')\n",
    "    p.remove_actor('date')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2796d-92df-4bb4-8799-1512f5436eb8",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5cdbec38-7e63-4fdb-aa8c-7c89d2bfe050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read netCDF surface_upward_sensible_heat_flux data. Shape=(460, 601, 1233)\n",
      "INFO:root:Read netCDF LAI data. Shape=(144, 602, 1293)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read datasets:\n",
    "arr_mesh_ref, lat_arr_mesh, long_arr_mesh, time_arr_mesh = load_nc(path_input_mesh)\n",
    "arr_color_ref, lat_arr_color, long_arr_color, time_arr_color = load_nc(path_input_color)\n",
    "\n",
    "lat_min, lat_max = find_min_max(lat_arr_mesh, \n",
    "                                lat_arr_color, \n",
    "                                crop_coordinates[:2] if crop_coordinates is not None else (np.nan, np.nan))\n",
    "long_min, long_max = find_min_max(long_arr_mesh, \n",
    "                                  long_arr_color, \n",
    "                                  crop_coordinates[2:] if crop_coordinates is not None else (np.nan, np.nan))\n",
    "\n",
    "bg, data_shape = make_BG_mesh(path_input_BG, bg_padding, \n",
    "                              lat_min, lat_max, long_min, long_max)\n",
    "\n",
    "events_per_time_idx_dict = get_events(event_csv_path, time_arr_mesh, data_shape,\n",
    "                                      lat_min, lat_max, long_min, long_max,\n",
    "                                     ) if event_csv_path is not None else {}\n",
    "\n",
    "\n",
    "if is_interp=='temporal':\n",
    "    arr_color = interpolate_nans_temporal(arr_color_ref)\n",
    "    \n",
    "p = create_plotter(frame_rate)\n",
    "p = add_BG(p, bg)\n",
    "\n",
    "## missing!! need to verify the range of the mesh and the color arrays.\n",
    "\n",
    "for i_time in range(5):#(n_timepoints):\n",
    "    \n",
    "    arr_mesh = load_arr_2D(arr_mesh_ref, i_time, min_zero=True)\n",
    "    arr_color = load_arr_2D(arr_color_ref, i_time)\n",
    "    \n",
    "    arr_mesh = get_world_part(arr_mesh, (lat_min, lat_max), (long_min, long_max),\n",
    "                              lat_arr_mesh, long_arr_mesh)\n",
    "    arr_color = get_world_part(arr_color, (lat_min, lat_max), (long_min, long_max),\n",
    "                               lat_arr_color, long_arr_color)\n",
    "    \n",
    "    arr_mesh = cv2.resize(arr_mesh, (data_shape[1]//cube_size, data_shape[0]//cube_size), \n",
    "                                     interpolation=cv2.INTER_NEAREST).T\n",
    "    arr_color = cv2.resize(arr_color, (data_shape[1]//cube_size, data_shape[0]//cube_size), \n",
    "                                        interpolation=cv2.INTER_NEAREST).T\n",
    "    \n",
    "    if is_interp=='spatial':\n",
    "        arr_color = interpolate_nans(arr_color)\n",
    "    \n",
    "    surf = make_surface(arr_mesh, cube_size, arr_color, clim)\n",
    "    \n",
    "    for iii in range(300//frame_rate):\n",
    "        if iii==0:\n",
    "            \n",
    "            p = remove_actors(p)\n",
    "\n",
    "            p.add_mesh(surf, scalars=\"colors\", cmap=cmap, clim=clim, show_edges=True, lighting=True, name='mesh')\n",
    "            if str(i_time) in events_per_time_idx_dict:\n",
    "                p.add_point_labels(events_per_time_idx_dict[str(i_time)][0],\n",
    "                                   events_per_time_idx_dict[str(i_time)][1],\n",
    "                                   name='event_text', italic=True, font_size=40,\n",
    "                                   shape_color='black', point_color='pink', point_size=40,\n",
    "                                   shape_opacity=0.5,\n",
    "                                   render_points_as_spheres=True, always_visible=True, shadow=True)\n",
    "            if i_time==0:\n",
    "                p.camera_position = 'xy'\n",
    "                #p.camera.position = (p.camera_position.position[0],p.camera_position.position[1],p.camera_position.position[2]+2000)\n",
    "                p.camera.zoom(2)\n",
    "                p.camera.azimuth -= 30\n",
    "                p.camera.elevation = -20\n",
    "        \n",
    "        p.camera.azimuth += 0.02\n",
    "        p.write_frame()\n",
    "\n",
    "pv.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d186720-b89c-4697-800a-40a6c1f86b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
