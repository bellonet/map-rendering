{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdcb307-7eef-49e2-aa37-eb234e6cb83f",
   "metadata": {},
   "source": [
    "To Do:  \n",
    "* move video rotation.\n",
    "* Legend.\n",
    "* Check dates competibilities color to mesh.\n",
    "* calc clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856896ab-e2dc-4e68-8e1f-788b0064cd04",
   "metadata": {},
   "source": [
    "test the basic functions needed to create the cube rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d693e55d-c244-4054-9492-8600a4f95672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import vtk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from skimage import io\n",
    "import cv2\n",
    "import logging\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import QhullError\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil.parser\n",
    "#import gemgis as gg\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8db744-72de-4a62-891a-98238dfe7b4a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2585bbcb-722d-4514-87c5-c11d0effb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_padding = 400\n",
    "cube_size = 10\n",
    "cmap = 'seismic' #'RdBu'# 'PiYG' 'seismic_r'\n",
    "clim = (-100,100)\n",
    "clim = (0,2.6)\n",
    "crop_coordinates = None # (40,55,0,20)\n",
    "\n",
    "n_timepoints=100\n",
    "frame_rate=30\n",
    "event_csv_path='events.csv'\n",
    "\n",
    "path_input_mesh = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "#path_input_color = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "path_input_color = '../../UFZ_RemoteSensing/NOAA-LAI-Europe-mon-2001-2012(1).nc'\n",
    "\n",
    "output_path=\"output_cube10_seismic_bgPad400.mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c97879-b318-47d0-936b-de6d6ba3b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_padding = 220\n",
    "cube_size = 20\n",
    "cmap = 'PiYG' #'RdBu'# 'PiYG' 'seismic_r'\n",
    "clim = (-100,100)\n",
    "clim = (0,2.6)\n",
    "lat_range = (40,59)\n",
    "long_range = (0,21)\n",
    "n_timepoints=100\n",
    "frame_rate=60\n",
    "event_csv_path=None#'events.csv'\n",
    "\n",
    "is_interp = 'spatial'\n",
    "\n",
    "path_input_mesh = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "#path_input_color = '../../UFZ_RemoteSensing/HOLAPS-H-JJA_anomaly-d-2001-2005.nc'\n",
    "path_input_color = '../../UFZ_RemoteSensing/NOAA-LAI-Europe-mon-2001-2012(1).nc'\n",
    "\n",
    "output_path=\"output_cube20_PiYG_bgPad200_cropped.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c78c83c-e2ac-4b2a-9c15-0100ebc40be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_BG = '../backgound_map_tests/bluemarble.png'\n",
    "n_interp = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb4c97-449f-45e7-879d-ee74721b5e17",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddfc2a-89dc-49f0-bbb5-8da1de432545",
   "metadata": {},
   "source": [
    "### Load, crop & resize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18d059b-1981-4505-b967-8f9eca5adb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nc(path_input):\n",
    "    ds = nc.Dataset(path_input)\n",
    "    arr_name = [n for n in ds.variables.keys() if ds[n].ndim==3][0]\n",
    "    arr_ref = ds[arr_name]\n",
    "    \n",
    "    logging.info(f'Read netCDF {arr_name} data. Shape={arr_ref.shape}')\n",
    "    \n",
    "    return ds, arr_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1817cc67-fc04-4ede-be5a-9c1059870611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time_arr_yearmonthday(ds):\n",
    "    \n",
    "    date_arr = ds['time'][:].astype(int)\n",
    "    \n",
    "    first_val = date_arr[0]\n",
    "    \n",
    "    if len(first_val.astype(str))!=8 or first_val.astype(str)[0] not in ['1','2']:\n",
    "\n",
    "        try:\n",
    "            start_date_obj = dateutil.parser.parse(ds.time_coverage_start)\n",
    "            end_date_obj = dateutil.parser.parse(ds.time_coverage_end)\n",
    "        except:\n",
    "            raise Exception(\"Can't parse netCDF dataset time format\")\n",
    "            \n",
    "        date_arr = [start_date_obj+timedelta(days=int(it-first_val)) for it in date_arr]\n",
    "        date_arr = np.array([int(it.strftime(\"%Y%m%d\")) for it in date_arr])\n",
    "            \n",
    "    return date_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a818719-b484-463c-96be-cb6149ecff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date_for_legend(date):\n",
    "    date_obj = datetime.strptime(date, '%Y%m%d')\n",
    "    return date_obj.strftime(\"%b %d, %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22da3d62-3bff-4bb0-941f-21e5a70f66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_delta(date1_int, date2_int):\n",
    "    return (datetime.strptime(str(date1_int), '%Y%m%d') - datetime.strptime(str(date2_int), '%Y%m%d')).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0511e23e-3315-4d1b-b3a9-6c20b9869bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mesh_dates_to_color_idx(dates_mesh, dates_color):\n",
    "    closest_smaller_idx_in_color = [np.where((dates_color-d)<=0)[0][-1] for d in dates_mesh]\n",
    "    distance_in_days_from_smaller = np.array([get_days_delta(d, dates_color[closest_smaller_idx_in_color[i]]) for i,d in enumerate(dates_mesh)])\n",
    "    distance_in_days_from_larger = np.array([get_days_delta(dates_color[closest_smaller_idx_in_color[i]+1], d) \n",
    "                                             if dates_color.size>closest_smaller_idx_in_color[i]+1 else np.inf\n",
    "                                             for i,d in enumerate(dates_mesh)])\n",
    "    \n",
    "    relative_distance_from_smaller = distance_in_days_from_smaller / (distance_in_days_from_smaller+distance_in_days_from_larger)\n",
    "    \n",
    "    idxs_as_float = np.array(closest_smaller_idx_in_color) + relative_distance_from_smaller\n",
    "    \n",
    "    return idxs_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7fc980-ab9b-4e3b-8da4-32a5dd490598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_between_slices(arr, arr_next, weight_next):\n",
    "    interp_arr = arr*(1-weight_next)+arr_next*weight_next\n",
    "    return interp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057d8d1d-43c4-4ab2-a49f-52f2282ed29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resize_arr(arr_ref, i_date, ds, is_min_zero, \n",
    "                        lat_min_max, long_min_max, \n",
    "                        data_shape, cube_size, is_interp=False):\n",
    "    \n",
    "    arr = load_arr_2D(arr_ref, int(i_date), is_min_zero)\n",
    "    \n",
    "    if not float(i_date).is_integer():\n",
    "        arr_next = load_arr_2D(arr_ref, int(i_date+1), is_min_zero)\n",
    "        arr = interpolate_between_slices(arr, arr_next, i_date%1)\n",
    "    \n",
    "    arr = get_world_part(arr, lat_min_max, long_min_max, ds)\n",
    "    arr = cv2.resize(arr, (data_shape[1]//cube_size, data_shape[0]//cube_size), \n",
    "                                     interpolation=cv2.INTER_NEAREST).T\n",
    "    \n",
    "    if is_interp:\n",
    "        arr = interpolate_nans(arr)\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18facf88-f055-41b3-98c4-85ae69be972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arr_2D(arr, arr_slice=0, min_zero=False, fill_val=-9999):\n",
    "    arr = arr[arr_slice]\n",
    "    arr = arr.filled(fill_value=np.nan)\n",
    "    arr = arr - np.nanmin(arr) if min_zero else arr\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf58070-929a-456c-9fac-fb9dab810212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(coor_arr_mesh, coor_arr_color, user_range):\n",
    "    \n",
    "    if user_range is None:\n",
    "        user_range = [np.nan, np.nan]\n",
    "        \n",
    "    coor_min = np.nanmax((np.nanmin(coor_arr_mesh), np.nanmin(coor_arr_color), user_range[0]))\n",
    "    coor_max = np.nanmin((np.nanmax(coor_arr_mesh), np.nanmax(coor_arr_color), user_range[1]))\n",
    "    \n",
    "    return coor_min, coor_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4292634e-7e00-4b6d-9983-fceec9ec23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_nearest(arr, val):\n",
    "    return (np.abs(arr - val)).argmin()\n",
    "\n",
    "def get_world_part(arr, lat_arr_src, long_arr_src, ds=None, padding=0):\n",
    "    \n",
    "    if ds is None:\n",
    "        lat_arr_dst = np.linspace(-90, 90, arr.shape[0])\n",
    "        long_arr_dst = np.linspace(-180, 180, arr.shape[1])\n",
    "    else:\n",
    "        lat_arr_dst = ds[\"latitude\"][:]\n",
    "        long_arr_dst = ds[\"longitude\"][:]\n",
    "\n",
    "    min_lat_idx = find_idx_nearest(lat_arr_dst, np.min(lat_arr_src))\n",
    "    max_lat_idx = find_idx_nearest(lat_arr_dst, np.max(lat_arr_src))\n",
    "\n",
    "    min_long_idx = find_idx_nearest(long_arr_dst, np.min(long_arr_src))\n",
    "    max_long_idx = find_idx_nearest(long_arr_dst, np.max(long_arr_src))\n",
    "\n",
    "    arr = arr[min_lat_idx-padding:max_lat_idx+padding, min_long_idx-padding:max_long_idx+padding]\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef53556-8e49-4799-aa1a-ef4050e337f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BG_mesh(path_input_BG, bg_padding, lat_min, lat_max, long_min, long_max):\n",
    "    bg = np.flip(io.imread(path_input_BG), axis=0)\n",
    "    bg = get_world_part(bg, (lat_min, lat_max), (long_min, long_max), padding=bg_padding)\n",
    "    data_shape = [s-bg_padding*2 for s in bg.shape[:2]]\n",
    "    path_bg = 'tmp.png'\n",
    "    io.imsave(path_bg, np.flip(bg,axis=0))\n",
    "    bg = pv.read(path_bg)\n",
    "    bg.origin = (-bg_padding, -bg_padding, 0)\n",
    "    \n",
    "    return bg, data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a96001-b992-4a58-9e81-f6059aa7fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nc_with_interp(arr_interp, ds, path_color_interp):\n",
    "    \n",
    "    var_names = list(ds.variables.keys())\n",
    "    arr_name = [n for n in ds.variables.keys() if ds[n].ndim==3][0]\n",
    "    \n",
    "    ncfile = nc.Dataset(path_color_interp , mode='w', format='NETCDF4_CLASSIC') \n",
    "    \n",
    "    time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).\n",
    "    lon_dim = ncfile.createDimension(var_names[1], ds[var_names[1]].shape[0])\n",
    "    lat_dim = ncfile.createDimension(var_names[2], ds[var_names[2]].shape[0])\n",
    "    \n",
    "    # Define two variables with the same names as dimensions,\n",
    "    # a conventional way to define \"coordinate variables\".\n",
    "    time = ncfile.createVariable(var_names[0], np.float64, (var_names[0],))\n",
    "    time.units = 'day as %Y%m%d.%f'\n",
    "    time.long_name = var_names[0]\n",
    "\n",
    "    lon = ncfile.createVariable(var_names[1], np.float32, (var_names[1],))\n",
    "    lon.units = 'degrees_east'\n",
    "    lon.long_name = var_names[1]\n",
    "\n",
    "    lat = ncfile.createVariable(var_names[2], np.float32, (var_names[2],))\n",
    "    lat.units = 'degrees_north'\n",
    "    lat.long_name = var_names[2]\n",
    "\n",
    "    # Define a 3D variable to hold the data\n",
    "    data = ncfile.createVariable(var_names[3],np.float32,\n",
    "                                 (var_names[0],var_names[2],var_names[1])) \n",
    "    # note: unlimited dimension is leftmost\n",
    "    data.units = 'W m-2'\n",
    "    data.standard_name = arr_name\n",
    "    \n",
    "    # fill in data\n",
    "    time[:] = ds[var_names[0]][:]\n",
    "    lon[:] = ds[var_names[1]][:]\n",
    "    lat[:] = ds[var_names[2]][:]\n",
    "    data[:] = arr_interp\n",
    "\n",
    "    ncfile.close(); \n",
    "    logging.info('Saved netCDF Dataset with interpolation!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10dba6bc-8dc2-4226-9be9-c9b92875fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(arr_color):\n",
    "    \n",
    "    if ~np.all(np.isnan(arr_color)):\n",
    "        \n",
    "        idxs_to_interp = np.where(np.isnan(arr_color))\n",
    "        idxs_interp_from = np.where(~np.isnan(arr_color))\n",
    "        \n",
    "        try:\n",
    "            arr_color[idxs_to_interp] = griddata(idxs_interp_from, \n",
    "                                             arr_color[idxs_interp_from], \n",
    "                                             idxs_to_interp)\n",
    "        except QhullError:\n",
    "            logging.warning('not interpulating - QHull error')\n",
    "            \n",
    "    return arr_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6d4790d-4cf2-4e78-aa41-db3be7681892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_interpolated_nans_temporal(arr_color_ref, ds_color, path_color_interp):\n",
    "    \n",
    "    arr_interp = np.full(arr_color_ref.shape, np.nan, dtype=np.float16)\n",
    "    \n",
    "    for i in range(arr_color_ref.shape[1]):\n",
    "        for j in range(arr_color_ref.shape[2]):\n",
    "            arr_color = load_arr_2D(arr_color_ref, np.index_exp[:,i,j])\n",
    "            arr_color = interpolate_nans(arr_color.astype(np.float16))\n",
    "        \n",
    "            arr_interp[:,i,j] = arr_color\n",
    "            \n",
    "    save_nc_with_interp(arr_interp, ds_arr_color, path_color_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fca87-e884-456e-b835-6718b50ae2f1",
   "metadata": {},
   "source": [
    "### Make plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8c41fd4-c121-4051-bb88-f8cf2757451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plotter(output_path, frame_rate):\n",
    "    p = pv.Plotter(notebook=False, off_screen=True,\n",
    "                  window_size=[1920 * 2, 1080 * 2], \n",
    "                   multi_samples=16, lighting='three lights')\n",
    "    p.open_movie(output_path, framerate=frame_rate)\n",
    "    p.set_background('black')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c29df8-1b83-4276-937b-2448bcf0bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_BG(p, bg):\n",
    "    floor_thickness = 40\n",
    "    p.add_mesh(pv.Cube(center=(bg.center[0], bg.center[1], -floor_thickness), \n",
    "                       x_length=bg.bounds[1]-bg.bounds[0],\n",
    "                       y_length=bg.bounds[3]-bg.bounds[2], \n",
    "                       z_length=floor_thickness*2-0.02),\n",
    "               color='white')\n",
    "\n",
    "    p.add_mesh(bg, rgb=True, scalars='PNGImage')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba8c62-3359-4311-ac0b-9476ec335fc6",
   "metadata": {},
   "source": [
    "### Make mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8b9247-a581-4d07-871e-5cb1da84e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_floor_vertices(arr, cube_size):\n",
    "\n",
    "    xs,ys = np.meshgrid(range(arr.shape[0]),range(arr.shape[1]))\n",
    "    \n",
    "    vertices_base = np.vstack((xs.T.flatten()*cube_size, \n",
    "                        ys.T.flatten()* cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "    \n",
    "    xs,ys = np.meshgrid(range(arr.shape[0]),[arr.shape[1]])\n",
    "    vertices_edge_y = np.vstack((xs.T.flatten()*cube_size, \n",
    "                        ys.T.flatten()*cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "    \n",
    "    xs,ys = np.meshgrid(arr.shape[0],range(arr.shape[1]))\n",
    "    vertices_edge_x = np.vstack((xs.T.flatten()* cube_size, \n",
    "                        ys.T.flatten()* cube_size,\n",
    "                        np.zeros(xs.size))).T\n",
    "        \n",
    "    return vertices_base, vertices_edge_y, vertices_edge_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31e40e90-86d8-44e9-86f9-380a69879d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all_vertices(arr, vertices_base, vertices_edge_y, vertices_edge_x, cube_size=1):\n",
    "    \n",
    "    vertices_ceil = vertices_base.copy()\n",
    "    vertices_ceil[:,2] = (arr.flatten()/10)**2-(np.nanmin(arr.flatten()/10)**2)\n",
    "    \n",
    "    vertices = np.concatenate((vertices_base,\n",
    "                               vertices_edge_y,\n",
    "                               vertices_edge_x,\n",
    "                               vertices_ceil,\n",
    "                               vertices_ceil+[0,1*cube_size,0],\n",
    "                               vertices_ceil+[1*cube_size,1*cube_size,0],\n",
    "                               vertices_ceil+[1*cube_size,0,0],\n",
    "                          ))\n",
    "    \n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9627dedf-eab8-42c7-8da7-127d29ec3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vertices(arr, cube_size=1):\n",
    "    vertices_base, vertices_edge_y, vertices_edge_x = make_floor_vertices(arr, cube_size)\n",
    "    vertices = concat_all_vertices(arr, vertices_base, vertices_edge_y, vertices_edge_x, cube_size)\n",
    "    \n",
    "    n_floor_v = vertices_base.shape[0]+vertices_edge_y.shape[0]+vertices_edge_x.shape[0]\n",
    "    return vertices, n_floor_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5d43a20-a588-43af-80a4-c5057687ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_floor_faces(arr):\n",
    "    faces = []\n",
    "    it = np.nditer(arr, flags=['c_index','multi_index'])\n",
    "\n",
    "    for x in it:\n",
    "        if not np.isnan(x):\n",
    "            \n",
    "            neigh_right_idx = it.index+1 if it.multi_index[1]!=arr.shape[1]-1 \\\n",
    "                                else arr.size + it.multi_index[0] \n",
    "            neigh_down_idx = it.index+arr.shape[1] if it.multi_index[0]!=arr.shape[0]-1 \\\n",
    "                                else arr.size + arr.shape[0] + it.multi_index[1]\n",
    "            \n",
    "            neigh_diag_idx = it.index+arr.shape[1]+1\n",
    "            if neigh_down_idx>arr.size:\n",
    "                neigh_diag_idx = neigh_down_idx+1\n",
    "            elif neigh_right_idx>arr.size-1:\n",
    "                neigh_diag_idx = neigh_right_idx+1\n",
    "\n",
    "            faces.append(np.array([4,\n",
    "                                   it.index,\n",
    "                                   neigh_right_idx,\n",
    "                                   neigh_diag_idx,\n",
    "                                   neigh_down_idx,\n",
    "                                  ]))\n",
    "            \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ed371b-86a9-4a62-b6fe-e4ae687353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ceil_faces(arr, n_v_floor):\n",
    "\n",
    "    faces = []\n",
    "    it = np.nditer(arr, flags=['c_index','multi_index'])\n",
    "\n",
    "    for x in it:\n",
    "        if not np.isnan(x):\n",
    "            faces.append(np.array([4,\n",
    "                                   n_v_floor+it.index,\n",
    "                                   n_v_floor+arr.size+it.index,\n",
    "                                   n_v_floor+arr.size*2+it.index,\n",
    "                                   n_v_floor+arr.size*3+it.index,\n",
    "                                  ]))\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd46421e-9c04-4b8a-abe4-5e51253445a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_side_faces(faces):\n",
    "    side_faces = []\n",
    "\n",
    "    for i in range(len(faces)//2):\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][1],\n",
    "                               faces[i][2],\n",
    "                               faces[len(faces)//2+i][2],\n",
    "                               faces[len(faces)//2+i][1],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][1],\n",
    "                               faces[i][4],\n",
    "                               faces[len(faces)//2+i][4],\n",
    "                               faces[len(faces)//2+i][1],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][2],\n",
    "                               faces[i][3],\n",
    "                               faces[len(faces)//2+i][3],\n",
    "                               faces[len(faces)//2+i][2],\n",
    "                              ]))\n",
    "        side_faces.append(np.array([4,\n",
    "                               faces[i][3],\n",
    "                               faces[i][4],\n",
    "                               faces[len(faces)//2+i][4],\n",
    "                               faces[len(faces)//2+i][3],\n",
    "                              ]))\n",
    "        \n",
    "    return side_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71b43e6f-8aae-42c1-a2e0-43f2d34b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_faces(arr, n_floor_v):\n",
    "    faces = make_floor_faces(arr)\n",
    "    faces.extend(make_ceil_faces(arr, n_floor_v))\n",
    "    faces.extend(make_side_faces(faces))\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da98a9eb-abe1-47c8-aa10-b5ba33fba794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_face_color(arr_color_flat, arr_mesh, clim):\n",
    "    arr_color_flat = np.delete(arr_color_flat, np.isnan(arr_mesh.flatten(order='F')))\n",
    "    \n",
    "    faces_color = np.concatenate((np.tile(arr_color_flat, 2),\n",
    "                               np.tile(arr_color_flat, (4,1)).flatten(order='F')))\n",
    "    \n",
    "#     arr_color_opacity = (np.abs(arr_color_flat-np.mean([clim[0],clim[1]])) / (np.ptp(clim)/2))*3\n",
    "    \n",
    "#     faces_opacity = np.concatenate((np.tile(arr_color_opacity, 2),\n",
    "#                                np.tile(arr_color_opacity, (4,1)).flatten(order='F')))    \n",
    "    \n",
    "#     faces_opacity[faces_opacity>1] = 1.0\n",
    "    \n",
    "#     faces_opacity = faces_opacity*0.8\n",
    "    \n",
    "    return faces_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c7ce4eb-b435-442b-b370-4963c5c922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_surface(arr_mesh, cube_size, arr_color, clim):\n",
    "    \n",
    "    vertices, n_floor_v = make_vertices(arr_mesh, cube_size)\n",
    "    faces = make_faces(arr_mesh, n_floor_v)\n",
    "    surf = pv.PolyData(vertices, faces)\n",
    "    faces_color = make_face_color(arr_color.flatten(order='F'), arr_mesh, clim)\n",
    "    surf[\"colors\"] = faces_color\n",
    "    \n",
    "    return surf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcfa72-23e9-4734-a7e0-1c114de0e9ea",
   "metadata": {},
   "source": [
    "### Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5a52c1-ac80-4ceb-9356-cc49c1abfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(event_csv_path, time_arr, data_shape, lat_min, lat_max, long_min, long_max):\n",
    "    \"\"\" Gets a csv file path of highlighted events that should be annotated in the rendering,\n",
    "    (as a text bubble on a specified map location) \n",
    "    it converts the data to x,y mesh position and timeframe indexing and returns it as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        event_csv_path: str path to csv file. Each row in the csv is an event. csv should have 5 columns:\n",
    "            \"first_date\" - first date of event in the format e.g. 20001231\n",
    "            \"last_date\" - last date of event\n",
    "            \"longitude\" - event longitude coordinate\n",
    "            \"latitude\" - event latitude coordinate\n",
    "            \"text\" - event text\n",
    "        time_arr: 1D array. Part of the netCDF dataset\n",
    "        long_arr: 1D array. Part of the netCDF dataset\n",
    "        lat_arr: 1D array. Part of the netCDF dataset\n",
    "\n",
    "    Returns:\n",
    "        event_dict - dict representation of the csv where each key is a timeframe index.\n",
    "                example: {\"0\":[[-20,250,150],\"event text example\"]}\n",
    "                means that there is only one event, it would be displayed at the first (0) timeframe,\n",
    "                at location [x,y,z] of the mesh coordinates.\n",
    "\n",
    "    Note:\n",
    "        for now also processing timepoints outside of user input timepoints.\n",
    "    TODO:\n",
    "        Need to check that the csv format is correct.\n",
    "        for now not checking that long and lat values are close to values in arrays:\n",
    "        e.g. [abs(long_arr[idx_long[i]]-val)<max_dist for i,val in enumerate(loc_arr[0])]\n",
    "    \"\"\"\n",
    "\n",
    "    lat_arr = np.linspace(lat_min, lat_max, data_shape[0])\n",
    "    long_arr = np.linspace(long_min, long_max, data_shape[1])\n",
    "        \n",
    "    event_dict = {}\n",
    "\n",
    "    df = pd.read_csv(event_csv_path, dtype={\"first_date\": str, \"last_date\": str})\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            idx_first_date = time_arr.index(df.loc[i, \"first_date\"])\n",
    "            idx_last_date = time_arr.index(df.loc[i, \"last_date\"])\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            logging.warning(f'event csv has an invalid date - row #{i}')\n",
    "            continue\n",
    "            \n",
    "        point = [(np.abs(long_arr - df.loc[i, \"longitude\"])).argmin(),\n",
    "                 (np.abs(lat_arr - df.loc[i, \"latitude\"])).argmin(),\n",
    "                 150]  # x,y,z\n",
    "        \n",
    "        if (not lat_min<point[0]<lat_max) or (not long_min<point[0]<long_max):\n",
    "            logging.warning(f'event csv has an invalid location - row #{i}')\n",
    "            continue\n",
    "                                    \n",
    "        text = (df.loc[i, \"text\"]).replace('\\\\n', '\\n')\n",
    "        for idx in range(idx_first_date, idx_last_date + 1):\n",
    "\n",
    "            if str(idx) not in event_dict:\n",
    "                event_dict[str(idx)] = [[point], [text]]\n",
    "            else:\n",
    "                event_dict[str(idx)][0].append(point)\n",
    "                event_dict[str(idx)][1].append(text)\n",
    "\n",
    "    return event_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e68da68-7498-4995-81d7-b07c12a44bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_actors(p):\n",
    "    p.remove_actor('mesh')\n",
    "    p.remove_actor('date')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2796d-92df-4bb4-8799-1512f5436eb8",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cdbec38-7e63-4fdb-aa8c-7c89d2bfe050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read netCDF surface_upward_sensible_heat_flux data. Shape=(460, 601, 1233)\n",
      "INFO:root:Read netCDF LAI data. Shape=(144, 602, 1293)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_mesh, arr_mesh_ref = load_nc(path_input_mesh)\n",
    "ds_color, arr_color_ref = load_nc(path_input_color)\n",
    "\n",
    "if is_interp=='temporal':\n",
    "    path_color_interp = f'{os.path.splitext(path_input_color)}_temporalInterpolation.nc'\n",
    "    if not os.path.exists(path_color_interp):\n",
    "        save_nc_with_interp(arr_color_ref, ds_color, path_color_interp)\n",
    "    ds_color, arr_color_ref = load_nc(path_color_interp)\n",
    "\n",
    "dates_mesh = format_time_arr_yearmonthday(ds_mesh)\n",
    "dates_color = format_time_arr_yearmonthday(ds_color)\n",
    "mesh_dates_idxs_in_color = map_mesh_dates_to_color_idx(dates_mesh, dates_color)\n",
    "\n",
    "lat_min, lat_max = find_min_max(ds_mesh[\"latitude\"][:], ds_color[\"latitude\"][:], lat_range)\n",
    "long_min, long_max = find_min_max(ds_mesh[\"longitude\"][:], ds_color[\"longitude\"][:], long_range)\n",
    "\n",
    "bg, data_shape = make_BG_mesh(path_input_BG, bg_padding, \n",
    "                              lat_min, lat_max, long_min, long_max)\n",
    "\n",
    "events_per_time_idx_dict = get_events(event_csv_path, time_arr_mesh, data_shape,\n",
    "                                      lat_min, lat_max, long_min, long_max,\n",
    "                                     ) if event_csv_path is not None else {}\n",
    "\n",
    "p = create_plotter(output_path, frame_rate)\n",
    "p = add_BG(p, bg)\n",
    "\n",
    "for i_date in range(n_timepoints-1):\n",
    "    \n",
    "    arr_mesh_curr = load_and_resize_arr(arr_mesh_ref, i_date, ds_mesh, True, \n",
    "                                        (lat_min, lat_max), (long_min, long_max), \n",
    "                                        data_shape, cube_size)\n",
    "    arr_mesh_next = load_and_resize_arr(arr_mesh_ref, i_date+1, ds_mesh, True, \n",
    "                                        (lat_min, lat_max), (long_min, long_max), \n",
    "                                        data_shape, cube_size)\n",
    "    \n",
    "    arr_color_curr = load_and_resize_arr(arr_color_ref, mesh_dates_idxs_in_color[i_date], \n",
    "                                   ds_color, False,\n",
    "                                  (lat_min, lat_max), (long_min, long_max),\n",
    "                                  data_shape, cube_size, is_interp=='spatial')\n",
    "    arr_color_next = load_and_resize_arr(arr_color_ref, mesh_dates_idxs_in_color[i_date+1], \n",
    "                                   ds_color, False,\n",
    "                                  (lat_min, lat_max), (long_min, long_max),\n",
    "                                  data_shape, cube_size, is_interp=='spatial')\n",
    "    \n",
    "    for i_interp in range(n_interp+1):\n",
    "        \n",
    "        p = remove_actors(p)\n",
    "        \n",
    "        arr_mesh = interpolate_between_slices(arr_mesh_curr, arr_mesh_next, i_interp/n_interp)\n",
    "        arr_color = interpolate_between_slices(arr_color_curr, arr_color_next, i_interp/n_interp)\n",
    "        \n",
    "        surf = make_surface(arr_mesh, cube_size, arr_color, clim)\n",
    "\n",
    "        p.add_mesh(surf, scalars=\"colors\", cmap=cmap, clim=clim, show_edges=True, lighting=True, name='mesh')\n",
    "        if str(i_date) in events_per_time_idx_dict:\n",
    "            p.add_point_labels(events_per_time_idx_dict[str(i_date)][0],\n",
    "                               events_per_time_idx_dict[str(i_date)][1],\n",
    "                               name='event_text', italic=True, font_size=40,\n",
    "                               shape_color='black', point_color='pink', point_size=40,\n",
    "                               shape_opacity=0.5,\n",
    "                               render_points_as_spheres=True, always_visible=True, shadow=True)\n",
    "            \n",
    "        if i_date==0 and i_interp==0:\n",
    "            p.camera_position = 'xy'\n",
    "            #p.camera.position = (p.camera_position.position[0],p.camera_position.position[1],p.camera_position.position[2]+2000)\n",
    "            p.camera.zoom(2)\n",
    "            p.camera.azimuth -= 30\n",
    "            p.camera.elevation = -20\n",
    "        \n",
    "        p.camera.azimuth += 0.02\n",
    "        p.write_frame()\n",
    "\n",
    "pv.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ed9ecd-fe34-4b6c-b700-aff4e6e32430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11.3.0 (conda-forge gcc 11.3.0-19)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-avresample --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1671040255947/_build_env/bin/pkg-config\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'output_cube20_PiYG_bgPad200_cropped.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:00:18.15, start: 0.000000, bitrate: 12774 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 3840x2160, 12768 kb/s, 60 fps, 60 tbr, 15360 tbn, 120 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> hevc (libx265))\n",
      "Press [q] to stop, [?] for help\n",
      "x265 [info]: HEVC encoder version 3.5+1-f0c1022b6\n",
      "x265 [info]: build info [Linux][GCC 10.3.0][64 bit] 8bit+10bit+12bit\n",
      "x265 [info]: using cpu capabilities: MMX2 SSE2Fast LZCNT SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "x265 [info]: Main profile, Level-5.1 (Main tier)\n",
      "x265 [info]: Thread pool created using 20 threads\n",
      "x265 [info]: Slices                              : 1\n",
      "x265 [info]: frame threads / pool features       : 4 / wpp(34 rows)\n",
      "x265 [info]: Coding QT: max CU size, min CU size : 64 / 8\n",
      "x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra\n",
      "x265 [info]: ME / range / subpel / merge         : hex / 57 / 2 / 2\n",
      "x265 [info]: Keyframe min / max / scenecut / bias  : 25 / 250 / 40 / 5.00 \n",
      "x265 [info]: Lookahead / bframes / badapt        : 15 / 4 / 0\n",
      "x265 [info]: b-pyramid / weightp / weightb       : 1 / 1 / 0\n",
      "x265 [info]: References / ref-limit  cu / depth  : 3 / on / on\n",
      "x265 [info]: AQ: mode / str / qg-size / cu-tree  : 2 / 1.0 / 32 / 1\n",
      "x265 [info]: Rate Control / qCompress            : CRF-32.0 / 0.60\n",
      "x265 [info]: tools: rd=2 psy-rd=2.00 rskip mode=1 signhide tmvp fast-intra\n",
      "x265 [info]: tools: strong-intra-smoothing lslices=8 deblock sao\n",
      "Output #0, mp4, to 'out1.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Video: hevc (hev1 / 0x31766568), yuv420p(progressive), 3840x2160, q=2-31, 60 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 libx265\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame= 1089 fps= 22 q=42.3 Lsize=   16454kB time=00:00:18.10 bitrate=7446.9kbits/s speed=0.36x    \n",
      "video:16438kB audio:0kB subtitle:0kB other streams:0kB global headers:2kB muxing overhead: 0.094619%\n",
      "x265 [info]: frame I:      5, Avg QP:32.70  kb/s: 48178.08\n",
      "x265 [info]: frame P:    214, Avg QP:35.04  kb/s: 18975.21\n",
      "x265 [info]: frame B:    870, Avg QP:42.22  kb/s: 4340.33 \n",
      "x265 [info]: Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "x265 [info]: consecutive B-frames: 0.5% 0.0% 0.5% 0.0% 99.1% \n",
      "\n",
      "encoded 1089 frames in 50.14s (21.72 fps), 7417.51 kb/s, Avg QP:40.77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffmpeg\n",
    "ffmpeg.input(output_path).output('out1.mp4', vcodec='libx265', preset='fast', crf=32).run()\n",
    "\n",
    "# tags explained\n",
    "#https://gist.github.com/tayvano/6e2d456a9897f55025e25035478a3a50:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49138b-3d71-4e84-a590-dfbab5de71f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
